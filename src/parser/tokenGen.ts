import { readFileSync } from 'fs';
import path from 'path';

interface TokenSpecToken { name: string; pattern: string; skip?: boolean }
interface TokenSpecCategory { name: string; tokens: TokenSpecToken[] }
interface TokenSpecRoot { categories: TokenSpecCategory[] }

export function loadTokenSpec(specPath: string): TokenSpecRoot {
  const raw = readFileSync(specPath, 'utf8');
  return JSON.parse(raw);
}

export function generateTokensSource(spec: TokenSpecRoot): string {
  const lines: string[] = [];
  lines.push('// AUTO-GENERATED BY scripts/generate_tokens.ts - DO NOT EDIT MANUALLY');
  lines.push("import { createToken, Lexer } from 'chevrotain';\n");
  for (const cat of spec.categories) {
    for (const t of cat.tokens) {
      const opts: string[] = [`name: '${t.name}'`, `pattern: /${t.pattern}/`];
      if (t.skip) opts.push('group: Lexer.SKIPPED');
      lines.push(`export const ${t.name} = createToken({ ${opts.join(', ')} });`);
    }
  }
  lines.push('\nexport const AllTokens = [');
  for (const cat of spec.categories) {
    for (const t of cat.tokens) lines.push(`  ${t.name},`);
  }
  lines.push('];');
  lines.push('\nexport const LocusLexer = new Lexer(AllTokens);\n');
  return lines.join('\n');
}

export function generateFromDefaultSpec(): string {
  const specPath = path.join(__dirname, 'token-spec.json');
  const spec = loadTokenSpec(specPath);
  return generateTokensSource(spec);
}
